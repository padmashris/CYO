---
title: Customer Churn Prediction using Logistic Regression, Classification Tree and Random Forest Models 
author: "Padmashri Saravanan"
date: "26 May 2020"
output:
  pdf_document:
    df_print: kable
    highlight: pygments
    number_sections: yes
    toc: yes
    toc_depth: 3
documentclass[twocolumn]: article
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

```{r installing libraries, include=FALSE}
# Install all needed packages if not already present
if(!require(tidyverse)) install.packages("tidyverse") 
if(!require(readr)) install.packages("readr")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyr)) install.packages("tidyr")
if(!require(corrplot)) install.packages("corrplot")
if(!require(caret)) install.packages("caret")
if(!require(rms)) install.packages("rms")
if(!require(e1071)) install.packages("e1071")
if(!require(MASS)) install.packages("MASS")
if(!require(ROCR)) install.packages("ROCR")
if(!require(randomForest)) install.packages("randomForest")
if(!require(gplots)) install.packages("gplots")
if(!require(pROC)) install.packages("pROC")
if(!require(psych)) install.packages("psych")
if(!require(rpart)) install.packages("rpart")
if(!require(ggpubr)) install.packages("ggpubr")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(kableExtra)) install.packages("kableExtra")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Load all the libraries

library(readr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
library(caret)
library(rms)
library(MASS)
library(e1071)
library(ROCR)
library(gplots)
library(pROC)
library(psych)
library(rpart)
library(randomForest)
library(ggpubr)
library(gridExtra)
library(kableExtra)
```

\newpage
# Overview

## Introduction to Churn Prediction
Churn prediction means detecting which customers are likely to cancel a subscription to a service based on how they use the service. It is a critical prediction for many businesses because acquiring new clients often costs more than retaining existing ones. Once you can identify those customers that are at risk of cancelling, you should know exactly what marketing action to take for each individual customer to maximise the chances that the customer will remain.

Different customers exhibit different behaviours and preferences, so they cancel their subscriptions for various reasons. It is critical, therefore, to proactively communicate with each of them in order to retain them in your customer list.

Harnessed properly, churn prediction can be a major asset in getting a clearer picture of your customers’ experience with your product. Although the range of potential factors behind churn can be complex, stopping churn often revolves around a tailored approach to improving customer experience. Churn prediction gives you the chance to improve a customer’s experience before they leave for good.

This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. In this analysis we predict the customer churning using Logistic Regression, Classification Tree and Random Forest algorithms and conclude that the logistic regression model and random forest model work better than the Classification Tree model. The accuracies are **0.78** for Logistic Regression, **0.78** for Classification Tree and **0.79** for Random Forest, with 0.5 as the threshold value. 

## The Dataset

**Source**

  <https://www.kaggle.com/shrutimechlearn/churn-modelling>


```{r message=FALSE, warning=FALSE, include=FALSE}
bank <- 
  read_csv("~/Downloads/Churn_Modelling.csv")
```

**Dimensions**
  
```{r echo=FALSE}
# Check dimensions of dataframe
tibble("Length" = nrow(bank), "Columns" = ncol(bank)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                position = "center",
                font_size = 10,
                full_width = FALSE)
```

**Variables**

```{r echo=FALSE}
# Getting the column names from the dataset
colnames(bank)
```


```{r, echo=TRUE}
# counting the customers who churn
sum(bank$Exited==1)
```

Thus, we have **2037** customers that churned, that's about 20%! Now, let's analyse the dataset to predict future customers that would churn using 3 different models.

# Methods & Analysis

## Exploring the Data

```{r include=FALSE}
bank <- tibble(bank)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
str(bank)
summary(bank)
```

### Cleaning the data

From the summary above, we can see that there are no missing values under any variable.

Now, we remove the `CustomerId` and `Surname` variable, since they won't be of any help for the analysis.

```{r, echo=TRUE}
bank_clean <- bank [,c(-2,-3)]
```

### Discrete Variables 

**Geography, Gender, Number of Products, Credit Card and Active Membership**
  
  We see that there are 3 classes in `Geography`,2 labels in `Gender` and 4 classes in `NumOfProducts`.

```{r, echo=TRUE}
unique(bank_clean$Gender)
```

```{r,echo=TRUE}
unique(bank_clean$Geography)
```

```{r,echo=TRUE}
unique(bank_clean$NumOfProducts)
```

**Checking the Churn Distributions**
  
We first convert any binary values to factors to help visualize the data.

```{r, echo=TRUE}
categorical <- bank_clean %>%
  mutate(Exited = ifelse(Exited==1,"Yes","No")) %>%
  mutate(HasCrCard = ifelse(HasCrCard==1,"Yes","No")) %>%
  mutate(IsActiveMember = ifelse(IsActiveMember==1,"Yes","No"))
```

Next, we check the churn rate (that is, the customers who have exited) against each variable.

```{r,echo=FALSE, fig.width=7}
c2 <- categorical %>%
  dplyr::select(Gender, Geography, NumOfProducts, HasCrCard, IsActiveMember, Exited)

p1 <- ggplot(c2)+
  geom_bar(aes(x = Gender, fill = Exited), position = "fill", stat = "count")

p2 <- ggplot(c2) +
  geom_bar(aes(x = Geography, fill = Exited), position = "fill", stat = "count") 

p3 <- ggplot(c2) +
  geom_bar(aes(x=NumOfProducts, fill=Exited),position="fill",stat="count")

p4 <- ggplot(c2) +
  geom_bar(aes(x=HasCrCard,fill=Exited),position="fill", stat="count")

p5 <- ggplot(c2) +
  geom_bar(aes(x=IsActiveMember,fill=Exited),position="fill", stat="count")

grid.arrange(p1,p2,p3,p4,p5,ncol=2)
```

We can clearly see that there are a greater number of females who Exited the service and customers from Germany Exited the most, followed by a close tie between France and Spain. It is also clear that customers with 4 products have the highest churn rate, followed by 3, 1 and 2 products. Customers having a credit card does not really affect the churn rate, but customers who are an active member have a lower churn rate than those who aren't.

\newpage
### Continuous Variables 

**Age, Credit Score, Tenure, Balance, Estimated Salary**

```{r,echo=FALSE,fig.width=7}
p6 <- ggplot(categorical,aes(Age,colour=Exited)) +
  geom_freqpoly(binwidth = 6,size = 0.8)

p7 <- ggplot(data=categorical,aes(CreditScore,colour=Exited)) +
  geom_freqpoly(binwidth = 25, size = 0.8)

p8 <- ggplot(data=categorical,aes(Tenure,colour=Exited)) +
  geom_freqpoly(binwidth = 2.35, size = 0.8)

p9 <- ggplot(data=categorical,aes(Balance,colour=Exited)) +
  geom_freqpoly(binwidth=25000, size=0.8)

p10 <- ggplot(data=categorical,aes(EstimatedSalary,colour=Exited)) +
  geom_freqpoly(binwidth=10000, size=0.8)

grid.arrange(p6,p7,p8,p9,p10,ncol=2)
```

The age of customers who exited (churned) are positively skewed, that is, customers who churned are more likely to close the account after the age of about 40 years. Contrastingly the customers who do not churn have a much higher peak, meaning a large group of current customers have been using the service till about 35 years of age.

We can notice that customers who do not churn with a credit score of 600 are extremely high and customers who churn with a score of 600 are relatively low. The tenure of customers who did not exit is negatively skewed, that is, customers with about 7 years tenure are less likely to close the account but customers who do churn also have about 7 years of tenure.

Both customers who churn and do not churn regardless of Balance seem to have a similar exit rate distribution, though the customers who have a lower amount of Balance is also seen to not have exited the bank's services. Lastly, we can see that there is no particular distribution for the `EstimatedSalary` variable.

**Correlations between Variables**
  
Now we check for correlations among the variables. 

```{r, fig.height=3,fig.width=5}
categorical %>%
  dplyr::select(Age, CreditScore, Tenure, Balance, EstimatedSalary) %>%
  cor() %>%
  corrplot.mixed(upper = "number", lower = "color", tl.col = "black", number.cex=2)
```

It is vivid that there is a negligible amount of correlation among the variables.

**Checking the Churn Rate for the complete dataset**
  
  ```{r echo= FALSE, warning = FALSE, message=FALSE,fig.align="center"}
bank_clean %>%
  summarise(Total = n(), Churn_count= sum(Exited == 1), Churn_probability = Churn_count/Total)
```

This tells us that there are about **20.3%** customers who churn!
  
## Logistic Regression Model
  
  Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.

### Data Cleaning

We first create dummy variables for all character variables, after converting back the `Exited` variable to binary values.

```{r,echo=TRUE}
bank_new <- categorical %>%
  mutate(Exited = ifelse(Exited=="Yes",1,0))
dummy <- dummyVars(" ~ .", data = bank_new)
dummy <- data.frame(predict(dummy, newdata = bank_new))
```

Now, we split the data into training and test sets (75% against 25%):
  
  ```{r echo= TRUE, warning = FALSE, message=FALSE}
set.seed(818)
assignment <- sample(0:1, size= nrow(dummy), prob = c(0.75,0.25), replace = TRUE)
train <- dummy[assignment == 0, ]
test <- dummy[assignment == 1, ]
```

Let's also examine if the churn rates of both sets are not too far off.

**The Training Set**
```{r echo= FALSE, warning = FALSE, message=FALSE}
train %>%
  summarise(Total = n(), Churn_count= sum(Exited == 1), Churn_probability = Churn_count/Total)
```

\newpage
**The Test Set**
```{r echo= FALSE, warning = FALSE, message=FALSE}
test %>%
  summarise(Total = n(), Churn_count = sum(Exited == 1), Churn_probability = Churn_count/Total)
```

### Training Set Models

We first use all columns to build the first model, `model1`. 
```{r echo= TRUE, warning = FALSE, message=FALSE}
model1 <- glm(Exited ~., family = "binomial", data = train)
```

Then we use AIC, to easily test the model’s performance, and to exclude variables based on their significance and create `model2`.

```{r echo= TRUE, warning = FALSE, message=FALSE}
model2 <- stepAIC(model1, trace = 0)
summary(model2)
```

The Variance Inflation Factor (VIF) is used to detect the presence of multicollinearity. Variance inflation factors (VIF) measure how much the variance of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related. Hence we use the VIF function to check for multicollinearity:

```{r echo= TRUE, warning = FALSE, message=FALSE}
vif(model2)
```

We see that all VIF values of `model2` are lesser than 2, but the p-value for `HasCrCardNo` is still relatively high, so we remove it to create `model3`:

```{r echo=TRUE}
model3 <- 
  glm(formula = Exited ~  CreditScore + GeographyGermany + GenderFemale
      + Age + Tenure + Balance + NumOfProducts + IsActiveMemberNo, 
      family = "binomial", data = train)
summary(model3)
```

Since this model does not seem to have any apparent discrepancies or issues, we use this as our final validation model to predict the churn rate on our training and test sets. 

### Cross Validation 

We set the default threshold value as 0.5.

```{r echo= TRUE, warning = FALSE, message=FALSE}
Lmodel <- model3
train_prob <- predict(Lmodel, data = train, type = "response") 
test_prob <- predict(Lmodel, newdata = test, type = "response")
train_pred <- factor(ifelse(train_prob >= 0.5, "Yes", "No"))
train_actual <- factor(ifelse(train$Exited == 1, "Yes", "No"))
test_pred <- factor(ifelse(test_prob >= 0.5, "Yes", "No"))
test_actual <- factor(ifelse(test$Exited == 1, "Yes", "No"))
```
Now, we compute the confusion matrix and ROC for both training and test sets.

**The Training Set**
```{r echo= TRUE, warning = FALSE, message=FALSE, fig.height=3}
confusionMatrix(data = train_pred, reference = train_actual)
roc <- roc(train$Exited, train_prob, plot= TRUE, print.auc=TRUE)
```

**The Test Set**
```{r echo= TRUE, warning = FALSE, message=FALSE,fig.height=3}
confusionMatrix(data = test_pred, reference = test_actual)
roc <- roc(test$Exited, test_prob, plot= TRUE, print.auc=TRUE)
```

Therefore we get the following table of results:

```{r echo=FALSE, fig.align='center'}
table <- matrix(c(0.8142,0.8038,0.2162,0.1827,0.9666,0.9646,0.765,0.780),ncol=2,byrow=TRUE)
rownames(table) <- c("Accuracy","Specificity","Sensitivity","AUC Value")
colnames(table) <- c("Training Set", "Test Set")
table %>% knitr::kable(align = 'c')
```

We then proceed to find the optimal threshold point that maximises the specificity and sensitivity.

### Finding the optimal cutoff

```{r echo= TRUE, warning = FALSE, message=FALSE}
pred <- prediction(train_prob, train_actual)
perf <- performance(pred, "spec", "sens")

thres <- data.frame(threshold=perf@alpha.values[[1]], specificity=perf@x.values[[1]], 
                      sensitivity= perf@y.values[[1]])
```

```{r echo= TRUE, fig.align='center'}
opt_thres <- thres[which.min(abs(thres$specificity-thres$sensitivity)),]
opt_thres %>% knitr::kable()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = thres) +
  geom_line(aes(x = threshold, y = specificity, color ="red"), size = 1.5)+
  geom_line(aes(x = threshold, y = sensitivity, color = "blue"), size = 1.5) +
  labs(x = "cutoff", y ="value") +
  scale_color_discrete(name = "", labels = c("Specificity", "Sensitivity"))+
  geom_vline(aes(xintercept = opt_thres$threshold))+
  geom_text(aes(x= 0.55, y= 0.75),label="opt_threshold = 0.2",hjust=1, size=4)
```

The optimal cutoff is 0.2. So I use it as the threshold to predict churn on training and test sets.

**Prediction on training set with threshold = 0.2:**

```{r echo= TRUE, warning = FALSE, message=FALSE}
train_pred_c <- factor(ifelse(train_prob >= 0.2, "Yes", "No"))
confusionMatrix(data = train_pred_c, reference = train_actual)
```

**Prediction on test set with threshold = 0.2:**

```{r echo= TRUE, warning = FALSE, message=FALSE}
test_prob <- predict(Lmodel, newdata = test, type = "response")
test_pred_c <- factor(ifelse(test_prob >= 0.2, "Yes", "No"))
confusionMatrix(data = test_pred_c, reference = test_actual)
```

For the training set, the Accuracy is 0.70, and the Sensitivity and Specificity are both about 0.70.
For the test set, the Accuracy is 0.70, and the Sensitivity and Specificity are both about 0.70 as well! Overall, this model with adjusted cutoff works well. 


### Summary for Logistic Regression Model

The final Logistic Regression Model (with threshold = 0.5) has Accuracy of 0.70 and the AUC is 0.78. Based on the P values for variables, `GeographyGermany`, `Tenure` and `NumOfProducts` have more significant influence on predicting churn.

## Classification Tree Model

Decision Trees are a class of very powerful Machine Learning model cable of achieving high accuracy in many tasks while being highly interpretable. What makes decision trees special in the realm of ML models is really their clarity of information representation. The “knowledge” learned by a Classification Tree through training is directly formulated into a hierarchical structure. This structure holds and displays the knowledge in such a way that it can easily be understood.

### Data Preparation
Classification Tree models can handle categorical variables without one-hot encoding them, and one-hot encoding will degrade tree-model performance. Thus, we re-prepare the data for Classification Tree and random forest models. We kept the "bank_clean" data before we do logistic regression and change the character variables to factors. Here's the final dataset we use for training classification tree models.


```{r echo= TRUE, warning = FALSE, message=FALSE}
banktree <- bank_clean
banktree <- banktree %>%
  mutate_if(is.character, as.factor)
str(banktree)
```


Split the data into training and test sets.
```{r echo= TRUE, warning = FALSE, message=FALSE}
set.seed(818)
tree <- sample(0:1, size= nrow(banktree), prob = c(0.75,0.25), replace = TRUE)
traintree <- banktree[tree == 0, ]
testtree <- banktree[tree == 1, ]
```

### Train Model1

First, we use all variables to build the model_tree1. 

```{r echo= TRUE, warning = FALSE, message=FALSE}
model_tree1 <- rpart(formula = Exited ~., data = traintree, 
                     method = "class", parms = list(split = "gini"))
```

### Cross Validation

```{r echo= TRUE, warning = FALSE, message=FALSE}
traintree_pred1 <- predict(model_tree1, data = traintree, type = "class")  
traintree_prob1 <- predict(model_tree1, data = traintree, type = "prob")  
testtree_pred1 <- predict(model_tree1, newdata= testtree, type = "class")  
testtree_prob1 <- predict(model_tree1, newdata = testtree, type = "prob") 
```

**For the Training Set**
```{r echo= TRUE, warning = FALSE, message=FALSE,fig.height=3}
confusionMatrix(data = as.factor(traintree_pred1), reference = as.factor(traintree$Exited))
traintree_actual <- ifelse(traintree$Exited==1,1,0)
roc <- roc(traintree_actual, traintree_prob1[,2], plot= TRUE, print.auc=TRUE)
```

**For the Test Set**
```{r echo= TRUE, warning = FALSE, message=FALSE,fig.height=3}
confusionMatrix(data = as.factor(testtree_pred1), reference = as.factor(testtree$Exited))
testtree_actual <- ifelse(testtree$Exited == 1, 1,0)
roc <- roc(testtree_actual, testtree_prob1[,2], plot = TRUE, print.auc = TRUE)
```

Hence, we get the following table of results:
  
```{r echo=FALSE, fig.align='center'}
table <- matrix(c(0.859,0.860,0.406,0.394,0.975,0.981,0.759,0.750),ncol=2,byrow=TRUE)
rownames(table) <- c("Accuracy","Specificity","Sensitivity","AUC Value")
colnames(table) <- c("Training Set", "Test Set")
table %>% knitr::kable(align = 'c')
```

Since each of the variables have negligible correlation, it is unlikely to affect the performance of the Classification Tree model. So we keep the first model as our final model.

### Summary for Classification Tree Model

The final Classification Tree model has Accuracy of **0.86** and AUC of **0.75** for the test set. It performs better than the logistic regression model, which had an Accuracy of **0.70** and AUC of **0.78** for the test set.

## Random Forest

Random forest is an ensemble tool which takes a subset of observations and a subset of variables to build a decision trees. It builds multiple such Classification Tree and amalgamate them together to get a more accurate and stable prediction. This is direct consequence of the fact that by maximum voting from a panel of independent judges, we get the final prediction better than the best judge.

### Data Preparation

We use the same data prepared for Classification Tree models.

### Train Model
```{r echo= FALSE, warning = FALSE, message=FALSE}
set.seed(802)
modelrf1 <- randomForest(formula = as.factor(Exited) ~., data = traintree)
print(modelrf1)
```

### Cross Validation
```{r echo= FALSE, warning = FALSE, message=FALSE}
trainrf_pred <- predict(modelrf1, traintree, type = "class")
trainrf_prob <- predict(modelrf1, traintree, type = "prob")  
testrf_pred <- predict(modelrf1, newdata = testtree, type = "class") 
testrf_prob <- predict(modelrf1, newdata = testtree, type = "prob") 
```

**For the Training Set:** 
```{r echo= FALSE, warning = FALSE, message=FALSE, fig.height=3}
confusionMatrix(data = as.factor(trainrf_pred), reference = as.factor(traintree$Exited))
trainrf_actual <- ifelse(traintree$Exited == 1, 1,0)
roc <- roc(trainrf_actual, trainrf_prob[,2], plot= TRUE, print.auc=TRUE)
```

**For the Test Set:**
```{r echo= FALSE, warning = FALSE, message=FALSE, fig.height=3}
confusionMatrix(data = as.factor(testrf_pred), reference = as.factor(testtree$Exited))
testrf_actual <- ifelse(testtree$Exited == 1, 1,0)
roc <- roc(testrf_actual, testrf_prob[,2], plot = TRUE, print.auc = TRUE)
```

Hence, we get the following table of results:
  
```{r echo=FALSE, fig.align='center'}
table <- matrix(c(1,0.86,1,0.452,1,0.966,1,0.856),ncol=2,byrow=TRUE)
rownames(table) <- c("Accuracy","Specificity","Sensitivity","AUC Value")
colnames(table) <- c("Training Set", "Test Set")
table %>% knitr::kable(align = 'c')
```


### Tuning 

#### Tuning mtry with tuneRF
```{r echo= TRUE, warning = FALSE, message=FALSE}
set.seed(818)
modelrf2 <- tuneRF(x = subset(traintree, select = -Exited), y = as.factor(traintree$Exited), ntreeTry = 500, doBest = TRUE)
print(modelrf2)
```

When mtry = 3, OOB decreases from 13.73% to 13.70%; when mtry = 6, OOB then increases to 13.88%.

#### Grid Search based on OOB error

We first establish a list of possible values for mtry, nodesize and sampsize.
```{r echo= TRUE, warning = FALSE, message=FALSE}
mtry <- seq(2, ncol(traintree) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(traintree) * c(0.7, 0.8)
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
```

Then, we create a loop to find the combination with the optimal 'oob err'. 
```{r echo= TRUE, warning = FALSE, message=FALSE}
oob_err <- c()
for (i in 1:nrow(hyper_grid)) {
  model <- randomForest(formula = as.factor(Exited) ~ ., 
                        data = traintree,
                        mtry = hyper_grid$mtry[i],
                        nodesize = hyper_grid$nodesize[i],
                        sampsize = hyper_grid$sampsize[i])
  oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

The optimal hyperparameters are mtry = 4, nodesize = 5, sampsize = 5230.4.

### Train model 2 with optimal hyperparameters.
```{r echo= TRUE, warning = FALSE, message=FALSE}
set.seed(802)
modelrf3 <- randomForest(formula = as.factor(Exited) ~., data = traintree, mtry = 4, nodesize = 5, sampsize = 5230.4)
print(modelrf3)
```

OOB of modelrf3 decreases a little bit to 13.6% with the optimal combination. The OOB of modelrf2 is 13.78%. 
So we will use modelrf3 as the final random forest model.

```{r}
trainrf_pred2 <- predict(modelrf2, traintree, type = "class")
trainrf_prob2 <- predict(modelrf2, traintree, type = "prob")  
testrf_pred2 <- predict(modelrf2, newdata = testtree, type = "class") 
testrf_prob2 <- predict(modelrf2, newdata = testtree, type = "prob") 
```

**For the Training Set:** 
```{r echo= FALSE, warning = FALSE, message=FALSE, fig.height=3}
confusionMatrix(data = as.factor(trainrf_pred2), reference = as.factor(traintree$Exited))
trainrf_actual <- ifelse(traintree$Exited == 1, 1,0)
roc <- roc(trainrf_actual, trainrf_prob2[,2], plot= TRUE, print.auc=TRUE)
```

**For the Test Set:**
```{r echo= FALSE, warning = FALSE, message=FALSE, fig.height=3}
confusionMatrix(data = as.factor(testrf_pred2), reference = as.factor(testtree$Exited))
testrf_actual <- ifelse(testtree$Exited == 1, 1,0)
roc <- roc(testrf_actual, testrf_prob2[,2], plot = TRUE, print.auc = TRUE)
```

Hence, we get the following table of results:
  
```{r echo=FALSE, fig.align='center'}
table <- matrix(c(1.000,0.859,1,0.452,1,0.965,1,0.855),ncol=2,byrow=TRUE)
rownames(table) <- c("Accuracy","Specificity","Sensitivity","AUC Value")
colnames(table) <- c("Training Set", "Test Set")
table %>% knitr::kable(align = 'c')
```

### Summary for Random Forest Model

The final random forest model has the Accuracy of 0.859 and AUC of 0.855 for the test set; the accuracy is higher than the Logistic Regression Model but really close to the Classification Tree Model. However, the Random Forest model has the highest AUC value among all 3 models.

\newpage
# Results

We can summarise the results obtained through all the models using the following table of results and the comparison of ROC and AUC for Logistic Regression, Classification Tree and Random Forest Models.

## Table of Results 

```{r echo=FALSE, fig.align='center'}
table <- matrix(c(0.703,0.860,0.859,0.700,0.394,0.452,0.704,0.981,0.965,0.780,0.750,0.855),ncol=3,byrow=TRUE)
rownames(table) <- c("Accuracy","Specificity","Sensitivity","AUC Value")
colnames(table) <- c("Logistic Regression", "Classification Tree","Random Forest")
table %>% knitr::kable(align = 'c')
```

## Comparison of ROC and AUC for Logistic Regression, Classification Tree and Random Forest models

```{r echo= TRUE, warning = FALSE, message=FALSE}
preds_list <- list(test_prob, testtree_prob1[,2],testrf_prob2[,2])
m <- length(preds_list)
actuals_list <- rep(list(testtree$Exited), m)

pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves for 3 Models")
legend(x = "bottomright",
       legend = c("Logistic Regression", "Classification Tree", "Random Froest"),
       fill = 1:m)
```

\newpage
# Discussion

We will now describe the metrics that we will compare in this section.

Accuracy is our starting point. It is the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.

Sensitivity is the number of True Positives divided by the number of True Positives and the number
of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Recall or the True Positive Rate. Sensitivity can be thought of as a measure of a classifiers completeness. A low sensitivity indicates many False Negatives.

Specificity (also called the true negative rate) measures the proportion of negatives which are correctly identified as such, and is complementary to the false positive rate. Specificity is also the number of true negatives divided by the sum of true negatives and false positives.

ROC (Receiver Operator Characteristic Curve) can help in deciding the best threshold value. It is generated by plotting the True Positive Rate against the False Positive Rate.

AUC stands for Area under the curve. AUC gives the rate of successful classification by the logistic model. The AUC makes it easy to compare the ROC curve of one model to another.

From the summary of results in the previous section it is clear that the Classification Tree Model has the greatest accuracy (0.860), followed by Logistic Regression having the greatest specificity (0.700), Classification Tree with the highest sensitivity (0.981) and Random Forest having the greatest AUC value (0.855).

# Conclusion 

This paper treats the Bank Customer Churn Analysis as a user classification problem.
In this report we investigated several machine learning model and we selected the optimal model by selecting a high accuracy level combinated with a low rate of false-negatives (high sensitivity).

The Random Forest model had the optimal results for Accuracy (0.859), Sensitivity (0.965) and
AUC value (0.855).

The analysis can also be further extended by exploring into other possible algorithms and models such as the Naive Bayes Model, KNN Model and Neural Networks. 

\newpage
# Appendix - Environment

```{r, echo=FALSE}
print("Operating System:")
version
```
